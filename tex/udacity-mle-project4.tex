\documentclass[12pt,letterpaper]{article}

\usepackage[bookmarks=true]{hyperref}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{color,colortbl}
\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{xtab}
\usepackage[final]{pdfpages}
\usepackage{subfigure}
\usepackage{amsmath,amssymb}


\definecolor{Gray}{gray}{0.9}

\defaultfontfeatures{Mapping=tex-text} % converts LaTeX specials (``quotes'' --- dashes etc.) to unicode

\setromanfont [Ligatures={Common}]{Cardo}
\setmonofont{Inconsolata}

% Set your name here
\def\name{Patrick J. Martin}

% Replace this with a link to your CV if you like, or set it empty
% (as in \def\footerlink{}) to remove the link in the footer:
\def\footerlink{}

% The following metadata will show up in the PDF properties
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\hypersetup{
  colorlinks = true,
  linkcolor=darkblue,
  urlcolor = darkblue,
  pdfauthor = {\name},
  pdftitle = {\name: PJM Application},
  pdfpagemode = UseNone
}

\geometry{
  body={6.5in, 8.5in},
  left=1.0in,
  top=1.0in,
  bottom=1.0in
}

\subfigcapmargin = .5cm

% Customize page headers
\pagestyle{myheadings}
\markright{\name}
\thispagestyle{empty}

% Custom section fonts
\usepackage{sectsty}
\sectionfont{\rmfamily\mdseries\Large}
\subsectionfont{\rmfamily\mdseries\large}

% Don't indent paragraphs.
\setlength\parindent{0em}

% Make lists without bullets
\renewenvironment{itemize}{
  \begin{list}{}{
    \setlength{\leftmargin}{1.5em}
  }
}{
  \end{list}
}

% courtesy D. Hovemeyer
\newenvironment{denseItemize}{%
\begin{list}{}{\setlength{\itemsep}{0.4em}\setlength{\leftmargin}{1.5em}\setlength{\parsep}{0in}}}{\end{list}}
%\setlength{\topsep}{.1mm}

\begin{document}

\pagestyle{myheadings}
\markright{\name---Project 4: Training a Smart Cab to Drive}
\thispagestyle{empty}

%{\LARGE \name} \\
%\smallskip
%\smallskip
{\Large Project 4: Training a Smart Cab to Drive} \\ 
Patrick Martin \\
\rule{\columnwidth}{1pt}

\vspace{1em}

In this project, I applied reinforcement learning techniques to design and simulate a smart cab agent operating.

\subsection*{Testing and Design Tasks}

To ensure that my development environment was complete, I implemented an agent that randomly selects an action without any ``smarts.'' 
This agent uses a discrete uniform distribution over the four possible actions: \verb|None|, \verb|left|, \verb|right|, and \verb|forward|.
It is clear from this strategy that the agent will bounce around the board due to the randomness of its action selection and the randomness of the environment (traffic lights and other agents).
The agent usually arrives at the final goal; however, there is no guarantee on timing performance. 
I let the agent run five times and tabulated the simulation deadline and the number of steps to completion:

\begin{center}
    \begin{tabular}{| c | c |}
    \hline
    \textbf{Deadline} & \textbf{Steps to Completion} \\ \hline
    20 & 168 \\ \hline
    40 & 120 \\ \hline
    25 & 7 \\ \hline
    30 & 22 \\ \hline
    40 & 118 \\
    \hline
    \end{tabular}
\end{center}

Two out of the five runs completed before the deadline, but it is likely due to the right combinations of initial conditions.
An open-loop algorithm will clearly not work for this task. \\

The driving agent state should be composed of environment variables that facilitate proper decision making.
The chosen state must balance the information required to solve the problem with the space complexity of the Q-learning algorithm.
Some input data may not be needed, which will create a more efficient learning algorithm. \\

I studied the available agent data and experimented with my implementation of the Q-learning algorithm.
At a minimum the state should include the intersection light, $\{\verb|green|,\verb|red|\}$, the direction of the next waypoint, $\{\verb|forward|,\verb|left|,\verb|right|\}$.
I added a new implicit state, the intersection traffic, based on the status of each path available to the agent.
A full state space would have used all three options for each direction; however, I chose to abstract them as a binary value: $\verb|true| := $ there is a vehicle on that path, $\verb|false|:=$ the path is clear.
Although I lose some information, the state space would have expanded to 384 states, which would take longer to explore.
I created the state space by building a Cartesian product in my Python code, \verb|itertools.product()|, resulting in 48 states.

%The other inputs, i.e. \verb|oncoming|, \verb|left|, \verb|right|, are encoded into the reward value when an action is taken.
%For example, if the agent decides to select the \verb|forward| action on a red light, it will be penalized with a value of -1.0.
%Using the state of \verb|light| and the set of possible actions, I construct the following (initial) Q-table:
%\begin{center}
%    \begin{tabular}{| c | c | c | c | c |}
%    \hline
%    \ & \verb|None| & \verb|forward| & \verb|left| & \verb|right| \\ \hline
%    \verb|green| & 0 & 0 & 0 & 0 \\ \hline
%    \verb|red| & 0 & 0 & 0 & 0 \\ \hline
%    \end{tabular}
%\end{center}

%Additionally, the agent might benefit from memory of its prior actions.
%My agent state will use a dictionary data structure with the following items:
%\begin{itemize}
%%	\item \verb|prior_action| - action chosen by the agent at the prior time tick
%	\item \verb|light| - the state of the traffic light at the intersection, red or green.
%	\item \verb|oncoming|, \verb|left|, \verb|right| - indicates if there is traffic oncoming, left, or right, respectively.
%	\item \verb|next_waypoint| - the agent's next goal waypoint
%	\item \verb|deadline| - the amount of time left for the deadline
%\end{itemize}
%The input data, the light and directional measurements, provide the agent knowledge about the intersection.
%For example, if the light is red, a smart agent should exclude the \verb|forward| action!
%The next waypoint and deadline states are needed to evolve the system learning, since they are related to higher level goals.

\end{document}